{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c6f8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T18:44:18.731566Z",
     "iopub.status.busy": "2024-12-23T18:44:18.731295Z",
     "iopub.status.idle": "2024-12-23T18:44:37.075895Z",
     "shell.execute_reply": "2024-12-23T18:44:37.074927Z"
    },
    "papermill": {
     "duration": 18.351133,
     "end_time": "2024-12-23T18:44:37.077621",
     "exception": false,
     "start_time": "2024-12-23T18:44:18.726488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 00:15:14.726662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-14 00:15:15.445257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.optim import SGD\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from going_modular.dataloader.multitask import create_multitask_datafetcher\n",
    "from going_modular.model.MTLFaceRecognition import MTLFaceRecognition\n",
    "from going_modular.loss.MultiTaskLoss import MultiTaskLoss\n",
    "from going_modular.train_eval.train import fit\n",
    "from going_modular.utils.transforms import RandomResizedCropRect, GaussianNoise\n",
    "from going_modular.utils.PolynomialLRWarmup import PolynomialLRWarmup\n",
    "from going_modular.utils.MultiMetricEarlyStopping import MultiMetricEarlyStopping\n",
    "from going_modular.utils.ModelCheckPoint import ModelCheckpoint\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Đặt seed toàn cục\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 99,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 0.2,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_spectacles_weight': 0.2,\n",
    "    'loss_da_spectacles_weight': 0.2,\n",
    "    'loss_occlusion_weight': 0.2,\n",
    "    'loss_da_occlusion_weight': 0.2,\n",
    "    'loss_facial_hair_weight': 0.2,\n",
    "    'loss_da_facial_hair_weight': 0.2,\n",
    "    'loss_pose_weight': 0.2,\n",
    "    'loss_da_pose_weight': 0.2,\n",
    "    'loss_gender_weight': 0.2,\n",
    "    'loss_da_gender_weight': 0.2,\n",
    "    'loss_emotion_weight': 0.2,\n",
    "    'loss_da_emotion_weight': 0.2,\n",
    "}\n",
    "\n",
    "CONFIGURATION['num_classes'] = len(os.listdir('./Dataset/Albedo/train'))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    RandomResizedCropRect(256),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    GaussianNoise(),\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=CONFIGURATION['image_size'], width=CONFIGURATION['image_size'])\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "train_dataloader, test_dataloader, train_weight_class = create_multitask_datafetcher(CONFIGURATION, train_transform, test_transform)\n",
    "model = MTLFaceRecognition(CONFIGURATION['backbone'], CONFIGURATION['num_classes'])\n",
    "\n",
    "criterion = MultiTaskLoss(os.path.join(CONFIGURATION['dataset_dir'], 'train_set.csv'), CONFIGURATION)\n",
    "optimizer = SGD(model.parameters(), lr=CONFIGURATION['base_lr'])\n",
    "scheduler = PolynomialLRWarmup(optimizer, warmup_iters=10, total_iters=CONFIGURATION['epochs'], power=2)\n",
    "\n",
    "earlystop_dir = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models')\n",
    "\n",
    "early_stopping = MultiMetricEarlyStopping(\n",
    "    monitor_keys=['cosine_auc', 'euclidean_auc'],\n",
    "    patience=1000,\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_dir=earlystop_dir,\n",
    "    start_from_epoch=0\n",
    ")      \n",
    "checkpoint_path = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models/checkpoint.pth')\n",
    "modle_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\ttrain: loss: 3.4070 | auc_cos: 0.6113 | auc_eu: 0.7032 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9963 | auc_occlusion: 0.9909 | auc_emotion: 0.9925\n",
      "\ttest: auc_cos: 0.7186 | auc_eu: 0.6186 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9993 | auc_occlusion: 0.9890 | auc_emotion: 0.9943\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 2:\n",
      "\ttrain: loss: 2.5841 | auc_cos: 0.7815 | auc_eu: 0.7803 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9596 | auc_occlusion: 0.9768 | auc_emotion: 0.9957\n",
      "\ttest: auc_cos: 0.6818 | auc_eu: 0.6416 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9599 | auc_occlusion: 0.9869 | auc_emotion: 0.9973\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 3:\n",
      "\ttrain: loss: 2.3394 | auc_cos: 0.8315 | auc_eu: 0.8188 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9843 | auc_occlusion: 0.9705 | auc_emotion: 0.9974\n",
      "\ttest: auc_cos: 0.6739 | auc_eu: 0.6640 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9924 | auc_occlusion: 0.9794 | auc_emotion: 0.9975\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 4:\n",
      "\ttrain: loss: 2.0836 | auc_cos: 0.8630 | auc_eu: 0.8536 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9887 | auc_occlusion: 0.9739 | auc_emotion: 0.9991\n",
      "\ttest: auc_cos: 0.7215 | auc_eu: 0.7079 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9971 | auc_occlusion: 0.9591 | auc_emotion: 0.9990\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 5:\n",
      "\ttrain: loss: 1.8720 | auc_cos: 0.8893 | auc_eu: 0.8773 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9889 | auc_occlusion: 0.9854 | auc_emotion: 0.9990\n",
      "\ttest: auc_cos: 0.7684 | auc_eu: 0.7337 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9913 | auc_occlusion: 0.9963 | auc_emotion: 0.9995\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 6:\n",
      "\ttrain: loss: 1.6128 | auc_cos: 0.8802 | auc_eu: 0.8870 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9642 | auc_occlusion: 0.9765 | auc_emotion: 0.9993\n",
      "\ttest: auc_cos: 0.8084 | auc_eu: 0.7863 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9911 | auc_occlusion: 0.9787 | auc_emotion: 0.9995\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 7:\n",
      "\ttrain: loss: 1.3875 | auc_cos: 0.9418 | auc_eu: 0.9424 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9774 | auc_occlusion: 0.9700 | auc_emotion: 0.9983\n",
      "\ttest: auc_cos: 0.8038 | auc_eu: 0.8018 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9932 | auc_occlusion: 0.9485 | auc_emotion: 0.9985\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 8:\n",
      "\ttrain: loss: 1.2315 | auc_cos: 0.9309 | auc_eu: 0.9433 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9603 | auc_occlusion: 0.9876 | auc_emotion: 0.9989\n",
      "\ttest: auc_cos: 0.8407 | auc_eu: 0.8366 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9948 | auc_occlusion: 0.9861 | auc_emotion: 0.9996\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 9:\n",
      "\ttrain: loss: 1.1456 | auc_cos: 0.9096 | auc_eu: 0.9228 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9825 | auc_occlusion: 0.8829 | auc_emotion: 0.9997\n",
      "\ttest: auc_cos: 0.8006 | auc_eu: 0.7740 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9960 | auc_occlusion: 0.9549 | auc_emotion: 1.0000\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 10:\n",
      "\ttrain: loss: 1.0480 | auc_cos: 0.8939 | auc_eu: 0.9178 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9943 | auc_occlusion: 0.9792 | auc_emotion: 0.9960\n",
      "\ttest: auc_cos: 0.8105 | auc_eu: 0.8030 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9964 | auc_occlusion: 0.9738 | auc_emotion: 0.9987\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 11:\n",
      "\ttrain: loss: 0.9358 | auc_cos: 0.9446 | auc_eu: 0.9653 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9806 | auc_occlusion: 0.9717 | auc_emotion: 0.9997\n",
      "\ttest: auc_cos: 0.9018 | auc_eu: 0.8982 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9918 | auc_occlusion: 0.9720 | auc_emotion: 0.9998\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 12:\n",
      "\ttrain: loss: 0.7490 | auc_cos: 0.9529 | auc_eu: 0.9623 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9725 | auc_occlusion: 0.9696 | auc_emotion: 0.9990\n",
      "\ttest: auc_cos: 0.7960 | auc_eu: 0.7920 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9884 | auc_occlusion: 0.9768 | auc_emotion: 0.9976\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 13:\n",
      "\ttrain: loss: 0.6470 | auc_cos: 0.9583 | auc_eu: 0.9636 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9908 | auc_occlusion: 0.9867 | auc_emotion: 0.9989\n",
      "\ttest: auc_cos: 0.8014 | auc_eu: 0.8170 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9955 | auc_occlusion: 0.9948 | auc_emotion: 0.9999\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 14:\n",
      "\ttrain: loss: 0.5365 | auc_cos: 0.9608 | auc_eu: 0.9714 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9875 | auc_occlusion: 0.9914 | auc_emotion: 0.9999\n",
      "\ttest: auc_cos: 0.8787 | auc_eu: 0.8803 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9964 | auc_occlusion: 0.9967 | auc_emotion: 0.9999\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 15:\n",
      "\ttrain: loss: 0.4705 | auc_cos: 0.9676 | auc_eu: 0.9790 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9953 | auc_occlusion: 0.9818 | auc_emotion: 0.9995\n",
      "\ttest: auc_cos: 0.8556 | auc_eu: 0.8937 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9968 | auc_occlusion: 0.9877 | auc_emotion: 0.9989\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 16:\n",
      "\ttrain: loss: 0.4248 | auc_cos: 0.9672 | auc_eu: 0.9755 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9873 | auc_occlusion: 0.9887 | auc_emotion: 0.9997\n",
      "\ttest: auc_cos: 0.8839 | auc_eu: 0.8924 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9884 | auc_occlusion: 0.9907 | auc_emotion: 0.9999\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 17:\n",
      "\ttrain: loss: 0.4160 | auc_cos: 0.9762 | auc_eu: 0.9805 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9837 | auc_occlusion: 0.9685 | auc_emotion: 0.9988\n",
      "\ttest: auc_cos: 0.8197 | auc_eu: 0.8813 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9859 | auc_occlusion: 0.9865 | auc_emotion: 0.9978\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 18:\n",
      "\ttrain: loss: 0.3829 | auc_cos: 0.9681 | auc_eu: 0.9792 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9910 | auc_occlusion: 0.9785 | auc_emotion: 0.9999\n",
      "\ttest: auc_cos: 0.8880 | auc_eu: 0.8878 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9989 | auc_occlusion: 0.9940 | auc_emotion: 1.0000\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 19:\n",
      "\ttrain: loss: 0.3629 | auc_cos: 0.9811 | auc_eu: 0.9871 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9752 | auc_occlusion: 0.9835 | auc_emotion: 0.9993\n",
      "\ttest: auc_cos: 0.8727 | auc_eu: 0.8823 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9846 | auc_occlusion: 0.9980 | auc_emotion: 0.9994\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 20:\n",
      "\ttrain: loss: 0.3289 | auc_cos: 0.9568 | auc_eu: 0.9772 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9866 | auc_occlusion: 0.9939 | auc_emotion: 0.9991\n",
      "\ttest: auc_cos: 0.8944 | auc_eu: 0.8963 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9865 | auc_occlusion: 0.9950 | auc_emotion: 0.9988\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 21:\n",
      "\ttrain: loss: 0.3384 | auc_cos: 0.9758 | auc_eu: 0.9763 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9857 | auc_occlusion: 0.9942 | auc_emotion: 0.9991\n",
      "\ttest: auc_cos: 0.8690 | auc_eu: 0.9010 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9935 | auc_occlusion: 0.9985 | auc_emotion: 0.9991\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n",
      "Epoch 22:\n",
      "\ttrain: loss: 0.3189 | auc_cos: 0.9762 | auc_eu: 0.9829 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9731 | auc_occlusion: 0.9807 | auc_emotion: 0.9997\n",
      "\ttest: auc_cos: 0.9071 | auc_eu: 0.8968 | auc_gender: 1.0000 | auc_spectacles: 1.0000 | auc_facial_hair: 1.0000 | auc_pose: 0.9862 | auc_occlusion: 0.9863 | auc_emotion: 0.9996\n",
      "\u001b[36m\tSaving model and optimizer state to /media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/checkpoint/multi/albedo/models/checkpoint.pth\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fit(\n",
    "    conf=CONFIGURATION,\n",
    "    start_epoch=0,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler, \n",
    "    early_stopping=early_stopping,\n",
    "    model_checkpoint=modle_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97f2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c888bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "\n",
    "image_path = './Dataset/Albedo/gallery/1002/2008-02-15_16-27-21.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de398517",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aab732d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5104, 0.4896]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "x_spectacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68474c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6338037,
     "sourceId": 10247612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17096.367763,
   "end_time": "2024-12-23T23:28:48.754749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T18:43:52.386986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
