{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c6f8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T18:44:18.731566Z",
     "iopub.status.busy": "2024-12-23T18:44:18.731295Z",
     "iopub.status.idle": "2024-12-23T18:44:37.075895Z",
     "shell.execute_reply": "2024-12-23T18:44:37.074927Z"
    },
    "papermill": {
     "duration": 18.351133,
     "end_time": "2024-12-23T18:44:37.077621",
     "exception": false,
     "start_time": "2024-12-23T18:44:18.726488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 01:07:07.952742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-14 01:07:08.670397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.optim import SGD\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from going_modular.dataloader.multitask import create_multitask_datafetcher\n",
    "from going_modular.model.MTLFaceRecognition import MTLFaceRecognition\n",
    "from going_modular.loss.MultiTaskLoss import MultiTaskLoss\n",
    "from going_modular.train_eval.train import fit\n",
    "from going_modular.utils.transforms import RandomResizedCropRect, GaussianNoise\n",
    "from going_modular.utils.WarmupCosineAnnealingWarmRestartsWithDecay import WarmupCosineAnnealingWarmRestartsWithDecay\n",
    "from going_modular.utils.MultiMetricEarlyStopping import MultiMetricEarlyStopping\n",
    "from going_modular.utils.ModelCheckPoint import ModelCheckpoint\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Đặt seed toàn cục\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "CONFIGURATION = {\n",
    "    'type': 'albedo',\n",
    "    \n",
    "    # Thư mục\n",
    "    'dataset_dir': './Dataset',\n",
    "    'checkpoint_dir': './checkpoint/multi/',\n",
    "    \n",
    "    # Cấu hình train\n",
    "    'device': device,\n",
    "    'epochs': 100,\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 256,\n",
    "    'base_lr': 0.2,\n",
    "    \n",
    "    # Cấu hình network\n",
    "    'backbone': 'miresnet18',\n",
    "    'embedding_size': 512,\n",
    "    'num_classes': None,\n",
    "    'loss_spectacles_weight': 0.2,\n",
    "    'loss_da_spectacles_weight': 0.2,\n",
    "    'loss_occlusion_weight': 0.2,\n",
    "    'loss_da_occlusion_weight': 0.2,\n",
    "    'loss_facial_hair_weight': 0.2,\n",
    "    'loss_da_facial_hair_weight': 0.2,\n",
    "    'loss_pose_weight': 0.2,\n",
    "    'loss_da_pose_weight': 0.2,\n",
    "    'loss_gender_weight': 0.2,\n",
    "    'loss_da_gender_weight': 0.2,\n",
    "    'loss_emotion_weight': 0.2,\n",
    "    'loss_da_emotion_weight': 0.2,\n",
    "}\n",
    "\n",
    "CONFIGURATION['num_classes'] = len(os.listdir('./Dataset/Albedo/train'))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    RandomResizedCropRect(256),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    GaussianNoise(),\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=CONFIGURATION['image_size'], width=CONFIGURATION['image_size'])\n",
    "], additional_targets={\n",
    "    'albedo': 'image',\n",
    "    'depthmap': 'image'\n",
    "})\n",
    "\n",
    "train_dataloader, test_dataloader, train_weight_class = create_multitask_datafetcher(CONFIGURATION, train_transform, test_transform)\n",
    "model = MTLFaceRecognition(CONFIGURATION['backbone'], CONFIGURATION['num_classes'])\n",
    "\n",
    "criterion = MultiTaskLoss(os.path.join(CONFIGURATION['dataset_dir'], 'train_set.csv'), CONFIGURATION)\n",
    "optimizer = SGD(model.parameters(), lr=CONFIGURATION['base_lr'])\n",
    "# Khởi tạo scheduler\n",
    "scheduler = WarmupCosineAnnealingWarmRestartsWithDecay(\n",
    "    optimizer,\n",
    "    warmup_iters=10,  # Số epoch cho warmup\n",
    "    T_0=30,          # Chu kỳ đầu tiên của cosine annealing\n",
    "    T_mult=2,        # Chu kỳ tăng gấp đôi sau mỗi lần restart\n",
    "    eta_min=1e-6,    # Giá trị learning rate nhỏ nhất\n",
    "    decay_factor=0.5 # Mỗi lần restart, base_lr giảm một nửa\n",
    ")\n",
    "\n",
    "earlystop_dir = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models')\n",
    "\n",
    "early_stopping = MultiMetricEarlyStopping(\n",
    "    monitor_keys=['cosine_auc', 'euclidean_auc'],\n",
    "    patience=1000,\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_dir=earlystop_dir,\n",
    "    start_from_epoch=0\n",
    ")      \n",
    "checkpoint_path = os.path.abspath(CONFIGURATION['checkpoint_dir'] + CONFIGURATION['type'] + '/models/checkpoint.pth')\n",
    "modle_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f42c339",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIGURATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodle_checkpoint\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/going_modular/train_eval/train.py:36\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(conf, start_epoch, model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, early_stopping, model_checkpoint)\u001b[0m\n\u001b[1;32m     32\u001b[0m device \u001b[38;5;241m=\u001b[39m conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 36\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     train_auc \u001b[38;5;241m=\u001b[39m compute_auc(train_dataloader, model, device)\n\u001b[1;32m     39\u001b[0m     train_gender_auc \u001b[38;5;241m=\u001b[39m train_auc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/media/vohoang/WorkSpace/ubuntu/projects/in-process/3d_face_recognition/going_modular/train_eval/train.py:154\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    150\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    152\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 154\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(\n",
    "    conf=CONFIGURATION,\n",
    "    start_epoch=0,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler, \n",
    "    early_stopping=early_stopping,\n",
    "    model_checkpoint=modle_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97f2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c888bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "\n",
    "image_path = './Dataset/Albedo/gallery/1002/2008-02-15_16-27-21.exr'\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path, cv2.IMREAD_UNCHANGED), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "transfromed = test_transform(image=image)\n",
    "\n",
    "X = torch.from_numpy(transfromed['image']).permute(2,0,1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de398517",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_id, x_gender, x_pose, x_emotion, x_facial_hair, x_occlusion, x_spectacles = model.get_embedding(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aab732d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8891, 0.1109]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_spectacles = torch.softmax(x_spectacles, dim=1)\n",
    "x_spectacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68474c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6338037,
     "sourceId": 10247612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17096.367763,
   "end_time": "2024-12-23T23:28:48.754749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T18:43:52.386986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
